{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8580fff9-03fb-4aec-86d8-feb2c1c8067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import keras\n",
    "from abc import ABC, abstractmethod\n",
    "from keras.datasets import cifar10\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d038738-fc64-4f75-8dc0-e8431ef29715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stabilization_point(\n",
    "    sequence,\n",
    "    window=5,\n",
    "    slope_threshold=0.005,\n",
    "    curvature_threshold=0.05,\n",
    "    patience=1,\n",
    "    oscillation_tolerance=0.001,\n",
    "    increasing_trend_threshold=0.01,\n",
    "    flat_change_threshold=0.003 \n",
    "):\n",
    "    if len(sequence) < window * 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    # Smooth the sequence using a moving average\n",
    "    smoothed_sequence = np.convolve(sequence, np.ones(window) / window, mode='valid')\n",
    "\n",
    "    # Calculate slopes and curvatures\n",
    "    slopes = np.diff(smoothed_sequence) / smoothed_sequence[:-1]\n",
    "    curvatures = np.diff(slopes)\n",
    "\n",
    "    stabilization_count = 0\n",
    "    for i in range(len(slopes) - window):\n",
    "        recent_slopes = slopes[i: i + window]\n",
    "        recent_curvatures = curvatures[i: i + window - 1]\n",
    "\n",
    "        # Stabilization conditions\n",
    "        is_stabilized = (\n",
    "            np.all(np.abs(recent_slopes) < slope_threshold) and\n",
    "            np.all(np.abs(recent_curvatures) < curvature_threshold)\n",
    "        )\n",
    "\n",
    "        # Oscillation detection\n",
    "        recent_values = smoothed_sequence[i: i + window]\n",
    "        oscillation_range = np.ptp(recent_values)\n",
    "        is_oscillating = oscillation_range < oscillation_tolerance\n",
    "\n",
    "        # Increasing trend detection\n",
    "        has_increasing_trend = np.all(recent_slopes > increasing_trend_threshold)\n",
    "\n",
    "        # Flat change detection\n",
    "        flat_change = np.abs(smoothed_sequence[i + window - 1] - smoothed_sequence[i]) < flat_change_threshold\n",
    "\n",
    "        if is_stabilized or is_oscillating or flat_change:\n",
    "            stabilization_count += 1\n",
    "            if stabilization_count >= patience:\n",
    "                return i + window\n",
    "        elif has_increasing_trend:\n",
    "            return i + window\n",
    "        else:\n",
    "            stabilization_count = 0\n",
    "\n",
    "    return float(\"inf\")\n",
    "\n",
    "class FloatSequenceTransformer(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, num_heads=1, num_layers=1, dropout=0.2):\n",
    "        super(FloatSequenceTransformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(1, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, \n",
    "                                                       batch_first=True, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(embedding_dim, 1)  # NO UNCERTAINTY\n",
    "        #self.output_layer = nn.Linear(embedding_dim, 2)\n",
    "\n",
    "    def generate_positional_encoding(self, seq_length, device):\n",
    "        position = torch.arange(seq_length, device=device).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2, device=device).float() * -(np.log(10000.0) / self.embedding_dim))\n",
    "        pe = torch.zeros(seq_length, self.embedding_dim, device=device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x, key_padding_mask):\n",
    "        seq_length = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        positional_encoding = self.generate_positional_encoding(seq_length, x.device)\n",
    "        x = x + 0.1 * positional_encoding # maybe positional encoding too heavily weighted!\n",
    "    \n",
    "        # Pass the 2D key_padding_mask\n",
    "        x = self.transformer(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = self.output_layer(x)\n",
    "        #mean, log_var = x[:, -1, 0], x[:, -1, 1]\n",
    "        #return mean, log_var\n",
    "        # NO UNCERTAINITY\n",
    "        return  x[:, -1, :]\n",
    "\n",
    "    def __str__(self):\n",
    "        current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "        s = (f\"transformer-model_emb{self.embedding_dim}_dropout{self.dropout}_layers{self.num_layers}_\"\n",
    "             f\"heads{self.num_heads}_date{current_date}\")\n",
    "        return s\n",
    "\n",
    "def predict_next_value(model, sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare the input sequence\n",
    "        input_sequence = torch.tensor(sequence, dtype=torch.float32).unsqueeze(-1)  # Shape: (seq_length, 1)\n",
    "        padded_input = torch.zeros(input_length, 1)  # Padding to fixed length\n",
    "        padded_input[:len(sequence)] = input_sequence  # Copy sequence into padded tensor\n",
    "\n",
    "        # Create the key_padding_mask\n",
    "        key_padding_mask = torch.full((1, input_length), True, dtype=torch.bool)  # All True initially\n",
    "        key_padding_mask[0, :len(sequence)] = False  # False for valid positions\n",
    "\n",
    "        # Make the prediction\n",
    "        padded_input = padded_input.unsqueeze(0)  # Add batch dimension: (1, input_length, 1)\n",
    "        \n",
    "        #mean, log_var = model(padded_input, key_padding_mask)\n",
    "        #uncertainty = torch.sqrt(torch.exp(log_var)).item()\n",
    "        #return mean.item(), uncertainty\n",
    "        # NO UNCERTAINTY\n",
    "        prediction = model(padded_input, key_padding_mask)\n",
    "        return prediction.item()\n",
    "\n",
    "# test uncertainty of model\n",
    "def predict_next_value_with_uncertainty(model, sequence, num_samples=60):\n",
    "    model.train()  # Set the model to training mode (this keeps dropout active)\n",
    "    predictions = []\n",
    "\n",
    "    # Prepare the input sequence\n",
    "    input_sequence = torch.tensor(sequence, dtype=torch.float32).unsqueeze(-1)  # Shape: (seq_length, 1)\n",
    "    padded_input = torch.zeros(input_length, 1)  # Padding to fixed length\n",
    "    padded_input[:len(sequence)] = input_sequence  # Copy sequence into padded tensor\n",
    "\n",
    "    # Create the key_padding_mask\n",
    "    key_padding_mask = torch.full((1, input_length), True, dtype=torch.bool)  # All True initially\n",
    "    key_padding_mask[0, :len(sequence)] = False  # False for valid positions\n",
    "\n",
    "    # Make the prediction\n",
    "    padded_input = padded_input.unsqueeze(0)  # Add batch dimension: (1, input_length, 1)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during inference\n",
    "        for _ in range(num_samples):\n",
    "            # Perform forward pass with dropout enabled\n",
    "            prediction = model(padded_input, key_padding_mask)\n",
    "            predictions.append(prediction.item())  # Store the predictions (detached from the computation graph)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    mean_prediction = predictions.mean(axis=0)  # Mean prediction across samples\n",
    "    std_prediction  = predictions.std(axis=0)  # Standard deviation as uncertainty\n",
    "    \n",
    "    return mean_prediction, std_prediction\n",
    "\n",
    "class TaskInterface(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        self.history = {'loss': []}\n",
    "        self.batches = []\n",
    "        self.lrs = []\n",
    "        self.batch_size = None\n",
    "        self.learning_rate = None\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load dataset and split into training and test sets\n",
    "        :return: tuple of (X_train, y_train), (X_test, y_test)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self) -> Model:\n",
    "        \"\"\"\n",
    "        Initializes and returns a compiled Keras model.\n",
    "\n",
    "        Returns:\n",
    "            Model: A compiled Keras model instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, **args):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, **args):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, **args):\n",
    "        pass\n",
    "\n",
    "class CIFAR10task(TaskInterface):\n",
    "\n",
    "    def __init__(self, save_path):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.batch_size = None\n",
    "        self.learning_rate = None\n",
    "        self.save_path = save_path\n",
    "        self.batches = [8 * i for i in range(1, 17)]\n",
    "        self.lrs = [0.001 * i for i in range(1, 11)]\n",
    "        self.history = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        (x_train, y_train) = (x_train[:len(x_train)//3], y_train[:len(x_train)//3])\n",
    "        self.x_train = x_train.astype(\"float32\") / 255.0\n",
    "        self.x_test = x_test.astype(\"float32\") / 255.0\n",
    "        self.y_train = keras.utils.to_categorical(y_train, 10)\n",
    "        self.y_test = keras.utils.to_categorical(y_test, 10)\n",
    "        print(f\"Loaded CIFAR10 dataset\\nShape X: {self.x_train.shape}\\nShape Y: {self.y_train.shape}\")\n",
    "\n",
    "    def evaluate(self, test_data=True):\n",
    "        results = self.model.evaluate(self.x_test, self.y_test, verbose=0) if test_data else self.model.evaluate(self.x_train, self.y_train, verbose=0)\n",
    "        return results\n",
    "\n",
    "    def train(self, epochs, callbacks=None):\n",
    "        history = self.model.fit(self.x_train,\n",
    "                                 self.y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 verbose=0,\n",
    "                                 callbacks=callbacks,\n",
    "                                 validation_data=(self.x_test, self.y_test))\n",
    "        for key in history.history:\n",
    "            if key not in self.history:\n",
    "                self.history[key] = []\n",
    "            self.history[key].extend(history.history[key])\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['accuracy'], label='Training Accuracy')\n",
    "        if 'val_accuracy' in self.history:\n",
    "            plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in self.history:\n",
    "            plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot to a file\n",
    "        img_path = f\"MNIST-lr{self.learning_rate}-bs{self.batch_size}.png\"\n",
    "        plt.savefig(img_path)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Conv2D(16, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(self.learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save(self.save_path)\n",
    "\n",
    "    def load_model(self, load_path):\n",
    "        self.model = keras.models.load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5764e8ea-8509-4ddc-8934-91f542756704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CIFAR10 dataset\n",
      "Shape X: (16666, 32, 32, 3)\n",
      "Shape Y: (16666, 10)\n"
     ]
    }
   ],
   "source": [
    "p = \"models/cnns_cifar10_categorical/transformer-model_emb8_dropout0.2_layers1_heads1_date27-12-2024.pth\"\n",
    "model = FloatSequenceTransformer() \n",
    "model.load_state_dict(torch.load(p))\n",
    "task = CIFAR10task(\"./test.model\")\n",
    "task.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68a3ab-cf1a-4830-8943-abd08607ecb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
