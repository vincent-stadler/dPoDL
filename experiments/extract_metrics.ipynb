{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c53862c-80a1-4e00-9359-571cbda49759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dda1262-00e0-4d15-9457-2ef338d5c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\daV\\Documents\\ZHAW\\HS 2024\\dPoDL\\dPoDL\\experiments\\github-downloads\")\n",
    "csvs = r\"C:\\Users\\daV\\Documents\\ZHAW\\HS 2024\\dPoDL\\dPoDL\\experiments\\github-csvs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d8c598-0fca-421d-ac64-bf5c169c23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual regex patterns for each metric for now just focus on relaxation of problem; LOSS\n",
    "epoch_loss_pattern = re.compile(r'epoch (\\d+)/\\d+.*? loss:\\s*(\\d+\\.\\d+)', re.DOTALL)\n",
    "#epoch_loss_val_loss_pattern1 = re.compile(r'epoch (\\d+)/\\d+.*? loss:\\s*(\\d+\\.\\d+).*? val_loss:\\s*(\\d+\\.\\d+)' , re.DOTALL)\n",
    "#epoch_loss_val_loss_pattern2 = re.compile(r'epoch (\\d+)/\\d+.*? val_loss:\\s*(\\d+\\.\\d+).*? loss:\\s*(\\d+\\.\\d+)', re.DOTALL)\n",
    "\n",
    "c = 0\n",
    "\n",
    "def regex_metrics_tf(text_original):\n",
    "    text_original = text_original.lower()\n",
    "    dfs = []\n",
    "    \n",
    "    # split everytime new epoch count is detected -> multiple trainings in one notebook\n",
    "    keyword = \"epoch 1/\"\n",
    "    text_splits = re.split(f'(?<={keyword})', text_original)[1:]    # Use regex with lookbehind to keep the keyword in the result\n",
    "    text_splits = [\"epoch 1/\" + s  for s in text_splits]\n",
    "    \n",
    "    for ix, text in enumerate(text_splits):\n",
    "        #start = time.time()\n",
    "        epoch_loss_matches = epoch_loss_pattern.findall(text)\n",
    "        #print(time.time()-start, \"first search\")\n",
    "        \n",
    "        if len(epoch_loss_matches) > 0:\n",
    "            df = pd.DataFrame(epoch_loss_matches, columns=[\"Epoch\", \"Loss\"])\n",
    "        else:\n",
    "            df = pd.DataFrame({})\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "\n",
    "def wrapper(filename):\n",
    "    file_out = os.path.splitext(filename)[0] + f\"-1.csv\"\n",
    "    file_out = os.path.join(csvs, file_out)\n",
    "    if os.path.isfile(file_out): \n",
    "        return\n",
    "    file = open(filename, \"r\").read()        \n",
    "    a = regex_metrics_tf(file)\n",
    "    for ix, df in enumerate(a):\n",
    "        if df.empty:\n",
    "            c += 1\n",
    "            #print(filename)\n",
    "            break\n",
    "        else:\n",
    "            file_out = file_out.replace(\"-1.csv\", f\"-{ix+1}.csv\")\n",
    "            df.to_csv(file_out, index=False)\n",
    "    return a\n",
    "\n",
    "\n",
    "#test_item = os.listdir()[2322]  #2322   sample 10  -> 82s\n",
    "#a = wrapper(test_item)\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(wrapper, os.listdir()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901907a9-8c21-4b1a-ba32-b85805e9dff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a577dde1-d00a-4123-ae0e-6b37d0dbdc5d",
   "metadata": {},
   "source": [
    "\n",
    "def regex_metrics_py(text, name):\n",
    "    text = text.lower()\n",
    "\n",
    "    iteration_matches= re.findall(iter_pattern_pytorch, text)\n",
    "    epoch_matches = re.findall(epoch_pattern_pytorch, text)\n",
    "    loss_matches = re.findall(loss_pattern_pytorch, text)\n",
    "    acc_matches = re.findall(acc_pattern_pytorch, text)\n",
    "    val_loss_matches = re.findall(val_loss_pattern_pytorch, text)\n",
    "    val_acc_matches = re.findall(val_acc_pattern_pytorch, text)\n",
    "\n",
    "    epoch_matches = [int(i) for i in epoch_matches]\n",
    "    epoch_matches = [i+1 for i in range(max(epoch_matches))]\n",
    "    \n",
    "    for metric in [val_loss_matches, val_acc_matches]:\n",
    "        if len(metric) != len(epoch_matches):\n",
    "            metric[:] = [None] * len(epoch_matches)\n",
    "        else:\n",
    "            metric[:] = [round(float(i)/100, 4) for i in metric]\n",
    "    \n",
    "    for metric in [loss_matches, acc_matches]:\n",
    "        if len(metric) != len(iteration_matches):\n",
    "            metric[:] = [None] * len(iteration_matches)\n",
    "        else:\n",
    "            metric[:] = [round(float(i)/100, 4) for i in metric]\n",
    "\n",
    "    last_loss = []\n",
    "    last_acc = []\n",
    "    step_size = len(loss_matches) // len(epoch_matches) if len(epoch_matches) > 0 else 1  # no division by 0\n",
    "    for ix in range(len(epoch_matches)):\n",
    "        try:\n",
    "            last_acc.append(acc_matches[ix * step_size])\n",
    "            last_loss.append(loss_matches[ix * step_size])\n",
    "        except IndexError:\n",
    "            last_loss.append(None)\n",
    "            last_acc.append(None)\n",
    "            \n",
    "    structured_data1 = list(zip(epoch_matches, val_acc_matches, val_loss_matches, last_acc, last_loss))\n",
    "    df1 = pd.DataFrame(structured_data1, columns=['Epoch', 'Val_Accuracy', 'Val_Loss', 'Accuracy', 'Loss'])\n",
    "    if len(df1) > 0:\n",
    "        print(f\"found {len(df1)} metrics for {name + 'epochs'}\")\n",
    "        #df1.to_csv(name + \"-epochs.csv\", index=False)\n",
    "\n",
    "    \n",
    "    structured_data2 = list(zip(iteration_matches, acc_matches, loss_matches))\n",
    "    df2 = pd.DataFrame(structured_data2, columns=['Iteration', 'Accuracy', 'Loss'])\n",
    "    if len(df2) > 0:\n",
    "        print(f\"found {len(df2)} metrics for {name + 'iterations'}\")\n",
    "        #df2.to_csv(name + \"-iterations.csv\", index=False)\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# Define individual regex patterns for each metric\n",
    "epoch_pattern_tf = r'epoch (\\d+)/\\d+'  \n",
    "loss_pattern_tf = r' loss:\\s*(\\d+\\.\\d+)'\n",
    "acc_pattern_tf = r' acc(?:uracy)?:\\s*(\\d+\\.\\d+)'\n",
    "val_loss_pattern_tf = r' val_loss:\\s*(\\d+\\.\\d+)'\n",
    "val_acc_pattern_tf = r' val_acc(?:uracy)?:\\s*(\\d+\\.\\d+)'\n",
    "\n",
    "iter_pattern_pytorch =  r'iter\\s*\\[(\\d+)\\s*/\\s*\\d+\\]'\n",
    "epoch_pattern_pytorch = r'epoch\\s*[\\[\\(]?\\s*(\\d+)\\s*/\\s*\\d+[\\]\\)]?' \n",
    "loss_pattern_pytorch = r' loss:\\s*(\\d+\\.\\d+)'\n",
    "acc_pattern_pytorch = r' top1:\\s*(\\d+\\.\\d+)'\n",
    "val_loss_pattern_pytorch = r' val_loss:\\s*(\\d+\\.\\d+)'\n",
    "val_acc_pattern_pytorch = r' val_top1:\\s*(\\d+\\.\\d+)'\n",
    "\n",
    "\n",
    "def regex_metrics_tf(text, name):\n",
    "    text = text.lower()\n",
    "\n",
    "    epoch_matches = re.findall(epoch_pattern_tf, text)\n",
    "    loss_matches = re.findall(loss_pattern_tf, text)\n",
    "    acc_matches = re.findall(acc_pattern_tf, text)\n",
    "    val_loss_matches = re.findall(val_loss_pattern_tf, text)\n",
    "    val_acc_matches = re.findall(val_acc_pattern_tf, text)\n",
    "    \n",
    "    for metric in [loss_matches, acc_matches, val_loss_matches, val_acc_matches]:\n",
    "        if len(metric) != len(epoch_matches):\n",
    "            metric[:] = [None] * len(epoch_matches)\n",
    "        else:\n",
    "            metric[:] = [float(i) for i in metric]\n",
    "    epoch_matches = [int(i) for i in epoch_matches]\n",
    "    \n",
    "    # List to hold the structured data\n",
    "    structured_data = list(zip(epoch_matches, acc_matches, loss_matches, val_acc_matches, val_loss_matches))\n",
    "    \n",
    "    # Create a DataFrame for better readability\n",
    "    df = pd.DataFrame(structured_data, columns=['Epoch', 'Accuracy', 'Loss', 'Val_Accuracy', 'Val_Loss'])\n",
    "    if len(df) > 0:\n",
    "        print(f\"found {len(df)} metrics for {name}\")\n",
    "        df.to_csv(name + \".csv\", index=False)\n",
    "        return df\n",
    "    else:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "470b12fa-2431-40ce-9ee2-f2c5ee562936",
   "metadata": {},
   "source": [
    "#DOWNLOAD RAW GITHUB\n",
    "\n",
    "l = []\n",
    "os.chdir(r\"C:\\Users\\daV\\Documents\\ZHAW\\HS 2024\\dPoDL\\dPoDL\\experiments\\github-links\")\n",
    "\n",
    "for i in os.listdir():\n",
    "    with open(i, \"r\") as f:\n",
    "        l.extend(f.readlines())\n",
    "\n",
    "print(len(l))\n",
    "\n",
    "os.chdir(r\"C:\\Users\\daV\\Documents\\ZHAW\\HS 2024\\dPoDL\\dPoDL\\experiments\\github-downloads\")\n",
    "not_found = failed = 0\n",
    "def download_file(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    get_url = \"https://raw.githubusercontent.com\" + parsed_url.path\n",
    "    file_name = parsed_url.path.split('/')[-1]\n",
    "    repo_name = \"--\".join(parsed_url.path.strip(\"/\").split('/')[:2])\n",
    "    file_name = repo_name + \"-\" + file_name\n",
    "\n",
    "    if os.path.isfile(file_name):\n",
    "        return\n",
    "    \n",
    "    response = requests.get(get_url)\n",
    "    if response.status_code == 200:\n",
    "        content = response.text\n",
    "        with open(file_name, \"w\") as file:\n",
    "            file.write(content)\n",
    "    elif response.status_code == 404:\n",
    "        continue\n",
    "    else:\n",
    "        failed += 1\n",
    "        print(f\"Failed to download: {url} - Status: {response.status_code}\")\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(download_file, set(l)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
