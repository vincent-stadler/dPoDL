{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c53862c-80a1-4e00-9359-571cbda49759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef82320-d2e7-4fa7-b446-c200f28863fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\daV\\Documents\\ZHAW\\HS 2024\\dPoDL\\dPoDL\\experiments\\filtered-search'\n",
    "\n",
    "# List all files\n",
    "all_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        all_files.append(file_path)\n",
    "directories = set([os.path.dirname(i) for i in all_files if not \"flagged\" in i])\n",
    "len(all_files)\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        #os.remove(file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67d8c598-0fca-421d-ac64-bf5c169c23ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileNotFound Nathan-Austin--Clothing_image_classifier_Tensorflow-Basic_20classification__20Classify_20images_20of_20clothing_20_C2_A0__C2_A0_20TensorFlow_20Core.html\n"
     ]
    }
   ],
   "source": [
    "# Define individual regex patterns for each metric for now just focus on relaxation of problem; LOSS\n",
    "epoch_loss_pattern = re.compile(r'epoch (\\d+)/\\d+.*? loss:\\s*(\\d+\\.\\d+)', re.DOTALL)\n",
    "#epoch_loss_val_loss_pattern1 = re.compile(r'epoch (\\d+)/\\d+.*? loss:\\s*(\\d+\\.\\d+).*? val_loss:\\s*(\\d+\\.\\d+)' , re.DOTALL)\n",
    "#epoch_loss_val_loss_pattern2 = re.compile(r'epoch (\\d+)/\\d+.*? val_loss:\\s*(\\d+\\.\\d+).*? loss:\\s*(\\d+\\.\\d+)', re.DOTALL)\n",
    "\n",
    "c = 0\n",
    "\n",
    "def regex_metrics_tf(text_original):\n",
    "    text_original = text_original.lower()\n",
    "    dfs = []\n",
    "    \n",
    "    # split everytime new epoch count is detected -> multiple trainings in one notebook\n",
    "    keyword = \"epoch 1/\"\n",
    "    text_splits = re.split(f'(?<={keyword})', text_original)[1:]    # Use regex with lookbehind to keep the keyword in the result\n",
    "    text_splits = [\"epoch 1/\" + s  for s in text_splits]\n",
    "    \n",
    "    for ix, text in enumerate(text_splits):\n",
    "        #start = time.time()\n",
    "        epoch_loss_matches = epoch_loss_pattern.findall(text)\n",
    "        #print(time.time()-start, \"first search\")\n",
    "        \n",
    "        if len(epoch_loss_matches) > 0:\n",
    "            df = pd.DataFrame(epoch_loss_matches, columns=[\"Epoch\", \"Loss\"])\n",
    "        else:\n",
    "            df = pd.DataFrame({})\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "\n",
    "def wrapper(filename):\n",
    "    file_out = os.path.splitext(filename)[0] + f\"-1.csv\"\n",
    "    if os.path.isfile(file_out): \n",
    "        return\n",
    "    file = open(filename, \"r\").read()        \n",
    "    a = regex_metrics_tf(file)\n",
    "    for ix, df in enumerate(a):\n",
    "        if df.empty:\n",
    "            print(\"empty:\", filename)\n",
    "            os.remove(filename)\n",
    "            break\n",
    "        else:\n",
    "            file_out = file_out.replace(\"-1.csv\", f\"-{ix+1}.csv\")\n",
    "            df.to_csv(file_out, index=False)\n",
    "    return a\n",
    "\n",
    "\n",
    "#test_item = os.listdir()[2322]  #2322   sample 10  -> 82s\n",
    "#a = wrapper(test_item)\n",
    "\n",
    "for directory in directories:\n",
    "    os.chdir(directory)\n",
    "    for file in [i for i in os.listdir() if (\"urls.txt\" not in i) and (\"flagged\" not in i) and (not i.endswith(\".csv\"))]:\n",
    "        try:\n",
    "            wrapper(file)\n",
    "        except FileNotFoundError:\n",
    "            print(\"FileNotFound\", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901907a9-8c21-4b1a-ba32-b85805e9dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER OUT POTENTIAL BAD ATTRIBUTION TO \"TRANSFORMERS\"\n",
    "import shutil\n",
    "cnn_keywords = [\"cnn\", \"convolutional\"]\n",
    "potential_bad = []\n",
    "for filename in all_files:\n",
    "\n",
    "    if \"urls.txt\" in filename:\n",
    "        continue\n",
    "    if \"filtered-search\\\\transformers\" in filename:\n",
    "        try:\n",
    "            with open(filename, \"r\") as f:\n",
    "                content = f.read().lower()\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "            \n",
    "        new_dir = os.path.join(os.path.dirname(filename), \"flagged\")\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        if any([True for i in cnn_keywords if i in content]) or any([True for i in cnn_keywords if i in filename]):\n",
    "            potential_bad.append(filename)\n",
    "            try:\n",
    "                shutil.move(filename, os.path.join(new_dir, os.path.basename(filename)))\n",
    "            except FileNotFoundError:\n",
    "                pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
